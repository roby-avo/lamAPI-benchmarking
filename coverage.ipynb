{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56057aa9-2c63-4043-8f80-4878cc54977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backoff in /opt/conda/lib/python3.11/site-packages (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffeea9db-4845-49a4-9c53-eb5f82205e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import re\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import backoff\n",
    "from tqdm.asyncio import tqdm\n",
    "from aiohttp.client_exceptions import ClientResponseError  # Add this import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca6421-7158-468a-8db8-7f23670fd1fc",
   "metadata": {},
   "source": [
    "# Round1_T2D_f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ef6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_file = './data/dataset_GT/Round1_T2D_f3.csv'\n",
    "\n",
    "chunk_size = 1000  # Adjust this based on your memory constraints\n",
    "ids = {}\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "total_rows = sum(1 for line in open(GT_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for chunk_GT in pd.read_csv(GT_file, chunksize=chunk_size):\n",
    "        items = chunk_GT[chunk_GT['target'] == 1]\n",
    "        for _, row in items.iterrows():\n",
    "            if row['target'] == 1:\n",
    "                ids[row[\"key\"]] = {\n",
    "                    \"id\": 'https://www.wikidata.org/entity/' + row['id'],\n",
    "                    \"name\": row['name'],\n",
    "                    \"ed_score\": row['ed_score'],\n",
    "                    \"jaccard_score\": row['jaccard_score']\n",
    "                }\n",
    "        pbar.update(1)  # Update progress bar for each chunk iteration\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc24f3-289d-4e50-a8e0-d2f7385a10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/Round1_T2D/gt/CEA_Round1_gt_WD.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = set(df[\"key\"].values)\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                key_to_cell[key] = cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019789e6-0ac5-4f77-9b9e-a68d9ccb44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round1_T2D/gt/CEA_Round1_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        parts = row['url'].split('/')\n",
    "        wikidata_id = parts[-1]\n",
    "        num_rows, num_columns = df.shape\n",
    "        key = f\"{row['table_name']} {row['row']} {row['col']}\"\n",
    "        if key in ids:\n",
    "            cell_value = key_to_cell[key]\n",
    "            data = ids[key]\n",
    "            mentions[cell_value] = data\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fcd61-91d8-4428-bf63-b4865ed7464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_mentions = sorted(mentions.items(), key=lambda x: x[1][\"ed_score\"])\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(sorted_mentions, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1e9063-4b83-49e1-97a6-e82d9ebfae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62327f8-cdfb-4c72-91d8-9a24c82ea65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ed_score and jaccard_score values\n",
    "ed_scores = [item[1]['ed_score'] for item in R1_sorted_mentions]\n",
    "jaccard_scores = [item[1]['jaccard_score'] for item in R1_sorted_mentions]\n",
    "\n",
    "# Convert to a pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame({'ED Score': ed_scores, 'Jaccard Score': jaccard_scores})\n",
    "\n",
    "# Density Plot for ED Score and Jaccard Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for ED Score\n",
    "sns.kdeplot(df['ED Score'], fill=True, label='ED Score')\n",
    "\n",
    "# Plot for Jaccard Score\n",
    "sns.kdeplot(df['Jaccard Score'], fill=True, label='Jaccard Score')\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ED and Jaccard Scores')\n",
    "plt.legend(loc='upper left')  # Show legend with labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac2be8-a8ef-48ef-846c-68c4df2ebdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcf5c8-742b-41d8-bc6b-6899751e3c71",
   "metadata": {},
   "source": [
    "## Sample extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39eea9d7-8d1b-4f56-9450-a8564fe1edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R1_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R1_sorted_mentions[:q1_idx]\n",
    "q2 = R1_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R1_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R1_sorted_mentions[q3_idx:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61221ee6-d837-453e-9e51-5240717cf1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = int(len(R1_sorted_mentions)/40)  \n",
    "R1_sample_keys = []\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q1, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q2, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q3, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q4, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb108a-226f-4247-9610-011db4e5184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting ED scores and Jaccard scores\n",
    "ed_scores = [score[1]['ed_score'] for score in R1_sample_keys]\n",
    "jaccard_scores = [score[1]['jaccard_score'] for score in R1_sample_keys]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.kdeplot(ed_scores, color='skyblue', label='Edit Distance Score', fill=True)\n",
    "sns.kdeplot(jaccard_scores, color='salmon', label='Jaccard Score', fill=True)\n",
    "\n",
    "plt.title('Edit Distance and Jaccard Score Density')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6316cb2-8719-4676-8b58-84c7f734a0a0",
   "metadata": {},
   "source": [
    "## Coverage Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c488ea-fe0b-4c9f-a842-803c6f60f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8264073-bb0e-46a4-b0f1-d93fccd98b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/608 [00:08<21:00,  2.09s/it]"
     ]
    }
   ],
   "source": [
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(query, session, el, string_name_list, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': str(el),\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': query\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{el}'\")\n",
    "            return 0, 0, False\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', string_name_list[el])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1, True\n",
    "\n",
    "    \n",
    "    ####################################\n",
    "    # FUZZY SEARCH\n",
    "    ####################################\n",
    "    #print(\"FUZZZZYYYYY\")\n",
    "    \n",
    "\n",
    "\n",
    "    return 0, 0, False\n",
    "\n",
    "async def main(R1_sample_keys, url, pbar):\n",
    "    string_name_list = {item[1]['name']: item[1]['id'] for item in R1_sample_keys}\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for el in string_name_list.keys():\n",
    "            query = f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{el}\", \"boost\": 2.0}}}}}}, {{\"range\": {{\"ntoken\": {{\"gte\": 0, \"lte\": 3}}}}}}]}}}}}}'\n",
    "            pbar.update(1)  # No need to await here\n",
    "            mrr_increment, count, flag = await process_item(query, session, el, string_name_list, url, headers, semaphore, pbar)\n",
    "            if flag:\n",
    "                tasks.append((mrr_increment, count))\n",
    "            else:\n",
    "                query = f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{str(el)}\", \"fuzziness\": \"AUTO\", \"boost\": 2.0}}}}}}, {{\"range\": {{\"ntoken\": {{\"gte\": 0, \"lte\": 3}}}}}}]}}}}}}'\n",
    "                mrr_increment, count, _ = await process_item(query, session, el, string_name_list, url, headers, semaphore, pbar)\n",
    "                tasks.append((mrr_increment, count))\n",
    "\n",
    "        print(tasks)\n",
    "        #results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for mrr_increment, count in tasks:\n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(R1_sample_keys)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(R1_sample_keys)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(R1_sample_keys))\n",
    "        await asyncio.run(main(R1_sample_keys, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(R1_sample_keys, url, pbar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82620c91-4d60-4136-95a8-f30bef19697f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cont_el' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoverage of R1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcont_el\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(R1_sample_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeasure Reciprocal Rank of R1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm_mrr\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(R1_sample_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cont_el' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Coverage of R1: {cont_el / len(R1_sample_keys)}\")\n",
    "print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(R1_sample_keys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed47fad-1b10-4bd7-82fa-5db7a54916a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 572/608 [00:32<00:02, 17.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of R1: 0.9407894736842105\n",
      "Measure Reciprocal Rank of R1: 0.919363486842111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, el, string_name_list, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': str(el),\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{el}\", \"boost\": 2.0}}}}}}]}}}}}}'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{el}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', string_name_list[el])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(R1_sample_keys, url, pbar):\n",
    "    string_name_list = {item[1]['name']: item[1]['id'] for item in R1_sample_keys}\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for el in string_name_list.keys():\n",
    "            tasks.append(process_item(session, el, string_name_list, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for mrr_increment, count in results:\n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(R1_sample_keys)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(R1_sample_keys)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(R1_sample_keys))\n",
    "        asyncio.run(main(R1_sample_keys, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(R1_sample_keys, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad92aafa-6ddf-4991-9e52-8ef02164fb15",
   "metadata": {},
   "source": [
    "### base query\n",
    "Coverage of R1: 0.962171052631579\n",
    "\r\n",
    "Measure Reciprocal Rank of 1T: 0.937271381578953\n",
    "\n",
    "### popularity based query\n",
    "Coverage of R1: 0.8552631578947368\n",
    "\r\n",
    "Measure Reciprocal Rank of R1: 0.82314144736842513"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65fe14-c716-462e-b419-55d9475e5555",
   "metadata": {},
   "source": [
    "# Round3_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dcd1b-493f-4de4-97b1-b57963feecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_file = './data/dataset_GT/Round3_f3.csv'\n",
    "chunk_size = 1000  # Adjust this based on your memory constraints\n",
    "\n",
    "ids = {}\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "total_rows = sum(1 for line in open(GT_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for chunk_GT in pd.read_csv(GT_file, chunksize=chunk_size):\n",
    "        items = chunk_GT[chunk_GT['target'] == 1]\n",
    "        for _, row in items.iterrows():\n",
    "            if row['target'] == 1:\n",
    "                ids[row[\"key\"]] = {\n",
    "                    \"id\": 'https://www.wikidata.org/entity/' + row['id'],\n",
    "                    \"name\": row['name'],\n",
    "                    \"ed_score\": row['ed_score'],\n",
    "                    \"jaccard_score\": row['jaccard_score']\n",
    "                }\n",
    "        pbar.update(1)  # Update progress bar for each chunk iteration\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e42600-5529-4f4c-82df-aef189b889d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/Round3_2019/gt/CEA_Round3_gt_WD.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = set(df[\"key\"].values)\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                key_to_cell[key] = cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e904f-c7c7-40a7-8660-3c95569810eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round3_2019/gt/CEA_Round3_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        parts = row['url'].split('/')\n",
    "        wikidata_id = parts[-1]\n",
    "        num_rows, num_columns = df.shape\n",
    "        key = f\"{row['table_name']} {row['row']} {row['col']}\"\n",
    "        if key in ids:\n",
    "            cell_value = key_to_cell[key]\n",
    "            data = ids[key]\n",
    "            mentions[cell_value] = data\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f2b55-1d9e-4594-ad61-12e2320d76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_mentions = sorted(mentions.items(), key=lambda x: x[1][\"ed_score\"])\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(sorted_mentions, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "465a6cc8-4d3b-4fc7-bf7a-7785600e3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R3_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc843c-3a51-4bc3-a761-bf6cf0406678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ed_score and jaccard_score values\n",
    "ed_scores = [item[1]['ed_score'] for item in R3_sorted_mentions]\n",
    "jaccard_scores = [item[1]['jaccard_score'] for item in R3_sorted_mentions]\n",
    "\n",
    "# Convert to a pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame({'ED Score': ed_scores, 'Jaccard Score': jaccard_scores})\n",
    "\n",
    "# Density Plot for ED Score and Jaccard Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for ED Score\n",
    "sns.kdeplot(df['ED Score'], fill=True, label='ED Score')\n",
    "\n",
    "# Plot for Jaccard Score\n",
    "sns.kdeplot(df['Jaccard Score'], fill=True, label='Jaccard Score')\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ED and Jaccard Scores')\n",
    "plt.legend(loc='upper left')  # Show legend with labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d50e2-a75d-4464-8d65-07058466d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abe44c-da17-47b6-8608-0eda5cf4bce3",
   "metadata": {},
   "source": [
    "## Sample extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f188f0a1-d65d-407c-995f-9bb659c15cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R3_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R3_sorted_mentions[:q1_idx]\n",
    "q2 = R3_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R3_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R3_sorted_mentions[q3_idx:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f19eb848-5633-4314-831c-0b1687a9bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = int(len(R3_sorted_mentions)/40) \n",
    "R3_sample_keys = []\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q1, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q2, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q3, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q4, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687287f-cc9f-4e97-99c9-e26f2bbc9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting ED scores and Jaccard scores\n",
    "ed_scores = [score[1]['ed_score'] for score in R3_sample_keys]\n",
    "jaccard_scores = [score[1]['jaccard_score'] for score in R3_sample_keys]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.kdeplot(ed_scores, color='skyblue', label='Edit Distance Score', fill=True)\n",
    "sns.kdeplot(jaccard_scores, color='salmon', label='Jaccard Score', fill=True)\n",
    "\n",
    "plt.title('Edit Distance and Jaccard Score Density')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ff417-c555-4121-a23f-9d741be9ca3c",
   "metadata": {},
   "source": [
    "## Coverage Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790a441-6042-4517-b278-474fedab80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "974723e2-9aeb-4113-9cc0-6474bc08b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 14902/15280 [12:50<00:19, 19.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of R1: 0.9752617801047121\n",
      "Measure Reciprocal Rank of R1: 0.9304014397904427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, el, string_name_list, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': str(el),\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{el}\", \"boost\": 2.0}}}}}}]}}}}}}'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{el}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', string_name_list[el])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(R3_sample_keys, url, pbar):\n",
    "    string_name_list = {item[1]['name']: item[1]['id'] for item in R3_sample_keys}\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for el in string_name_list.keys():\n",
    "            tasks.append(process_item(session, el, string_name_list, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for mrr_increment, count in results:\n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(R3_sample_keys)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(R3_sample_keys)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(R3_sample_keys))\n",
    "        asyncio.run(main(R3_sample_keys, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(R3_sample_keys, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d52bb4-d962-40e8-a6d0-bc8c0e388975",
   "metadata": {},
   "source": [
    "### base query\n",
    "Coverage of R3: 0.974738219895288\n",
    "\r\n",
    "Measure Reciprocal Rank of R3: 0.929046596858504\n",
    "\n",
    "### popularity base query\n",
    "\n",
    "Coverage of R1: 0.9752617801047121\n",
    "\r\n",
    "Measure Reciprocal Rank of R1: 0.93040143979044272"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a53e3-e5fe-46d9-bc52-244e7802b197",
   "metadata": {},
   "source": [
    "# 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21682bc5-8812-4cef-b596-0160c59421c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_file = './data/dataset_GT/2T-2020_f3.csv'\n",
    "chunk_size = 1000  # Adjust this based on your memory constraints\n",
    "\n",
    "ids = {}\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "total_rows = sum(1 for line in open(GT_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for chunk_GT in pd.read_csv(GT_file, chunksize=chunk_size):\n",
    "        items = chunk_GT[chunk_GT['target'] == 1]\n",
    "        for _, row in items.iterrows():\n",
    "            ids[row[\"key\"]] = {\n",
    "                \"id\": 'https://www.wikidata.org/entity/' + row['id'],\n",
    "                \"name\": row['name'],\n",
    "                \"ed_score\": row['ed_score'],\n",
    "                \"jaccard_score\": row['jaccard_score']\n",
    "            }\n",
    "        pbar.update(1)  # Update progress bar for each chunk iteration\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b44b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/2T_2020/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/2T_2020/gt/cea.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = set(df[\"key\"].values)\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                key_to_cell[key] = cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3b254-fd9b-4b32-a799-5c243c8daedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/2T_2020/gt/cea.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        parts = row['url'].split('/')\n",
    "        wikidata_id = parts[-1]\n",
    "        num_rows, num_columns = df.shape\n",
    "        key = f\"{row['table_name']} {row['row']} {row['col']}\"\n",
    "        if key in ids:\n",
    "            cell_value = key_to_cell[key]\n",
    "            data = ids[key]\n",
    "            mentions[cell_value] = data\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee2419-0618-4f43-88b4-087bfa1b8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_mentions = sorted(mentions.items(), key=lambda x: x[1][\"ed_score\"])\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(sorted_mentions, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c16bb8b0-9ae2-4774-87f5-4b0b2a3170a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_2T_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05332528-eedf-4027-bfda-d5d9e784c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ed_score and jaccard_score values\n",
    "ed_scores = [item[1]['ed_score'] for item in R4_2T_sorted_mentions]\n",
    "jaccard_scores = [item[1]['jaccard_score'] for item in R4_2T_sorted_mentions]\n",
    "\n",
    "# Convert to a pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame({'ED Score': ed_scores, 'Jaccard Score': jaccard_scores})\n",
    "\n",
    "# Density Plot for ED Score and Jaccard Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for ED Score\n",
    "sns.kdeplot(df['ED Score'], fill=True, label='ED Score')\n",
    "\n",
    "# Plot for Jaccard Score\n",
    "sns.kdeplot(df['Jaccard Score'], fill=True, label='Jaccard Score')\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ED and Jaccard Scores')\n",
    "plt.legend(loc='upper left')  # Show legend with labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb8cf3-f85a-4dc1-a5ce-19455a021584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681548a-3abe-4812-97d9-3bcee7163222",
   "metadata": {},
   "source": [
    "## Sample extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db422eb9-a3ab-431c-b932-78e92497ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_2T_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_2T_sorted_mentions[:q1_idx]\n",
    "q2 = R4_2T_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_2T_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_2T_sorted_mentions[q3_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119033ab-6741-4ada-8d44-c2925cf6cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = int(len(R4_2T_sorted_mentions)/40) \n",
    "R4_2T_sample_keys = []\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q1, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q2, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q3, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q4, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cf738-ad6c-43bd-b6e9-967615fa8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting ED scores and Jaccard scores\n",
    "ed_scores = [score[1]['ed_score'] for score in R4_2T_sample_keys]\n",
    "jaccard_scores = [score[1]['jaccard_score'] for score in R4_2T_sample_keys]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.kdeplot(ed_scores, color='skyblue', label='Edit Distance Score', fill=True)\n",
    "sns.kdeplot(jaccard_scores, color='salmon', label='Jaccard Score', fill=True)\n",
    "\n",
    "plt.title('Edit Distance and Jaccard Score Density')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714da0e-7477-4bf5-bf59-2f0f45881078",
   "metadata": {},
   "source": [
    "## Coverage Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767ab4c-ac3f-488e-9115-b041dbbca58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "480ac852-dca4-4689-b45e-0d21b5dff1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 3101/6528 [02:16<02:30, 22.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of R1: 0.47503063725490197\n",
      "Measure Reciprocal Rank of R1: 0.4591340379901872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, el, string_name_list, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': str(el),\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{el}\", \"boost\": 2.0}}}}}}]}}}}}}'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{el}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', string_name_list[el])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(R4_2T_sample_keys, url, pbar):\n",
    "    string_name_list = {item[1]['name']: item[1]['id'] for item in R4_2T_sample_keys}\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for el in string_name_list.keys():\n",
    "            tasks.append(process_item(session, el, string_name_list, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for mrr_increment, count in results:\n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(R4_2T_sample_keys)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(R4_2T_sample_keys)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(R4_2T_sample_keys))\n",
    "        asyncio.run(main(R4_2T_sample_keys, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(R4_2T_sample_keys, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258d622-4602-458c-bd9e-802885c56bac",
   "metadata": {},
   "source": [
    "### base query \n",
    "Coverage of R4_2T: 0.48713235294117646\n",
    "\n",
    "Measure Reciprocal Rank of R4_2T: 0.4726035539215572\n",
    "\n",
    "### popularity based query \n",
    "Coverage of R1: 0.47503063725490197\n",
    "\r\n",
    "Measure Reciprocal Rank of R1: 0.4591340379901872"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60955944",
   "metadata": {},
   "source": [
    "# Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3793c96-1cdd-48fb-aaca-1c84d7ed1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_file = './data/dataset_GT/Round4_f3.csv'\n",
    "chunk_size = 1000  # Adjust this based on your memory constraints\n",
    "\n",
    "ids = {}\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "total_rows = sum(1 for line in open(GT_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for chunk_GT in pd.read_csv(GT_file, chunksize=chunk_size):\n",
    "        items = chunk_GT[chunk_GT['target'] == 1]\n",
    "        for _, row in items.iterrows():\n",
    "            \n",
    "            if row['name'] == \"imo 9528017\":\n",
    "                print(row)\n",
    "                break\n",
    "\n",
    "            \n",
    "            ids[row[\"key\"]] = {\n",
    "                \"id\": 'https://www.wikidata.org/entity/' + row['id'],\n",
    "                \"name\": row['name'],\n",
    "                \"ed_score\": row['ed_score'],\n",
    "                \"jaccard_score\": row['jaccard_score']\n",
    "            }\n",
    "        pbar.update(1)  # Update progress bar for each chunk iteration\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a3d73-a6d9-4537-bf8d-02d55391fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/Round4_2020/gt/cea.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = set(df[\"key\"].values)\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                key_to_cell[key] = cell_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17cd3ec-c29e-48c4-8db9-3090849293f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round4_2020/gt/cea.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        parts = row['url'].split('/')\n",
    "        wikidata_id = parts[-1]\n",
    "        num_rows, num_columns = df.shape\n",
    "        key = f\"{row['table_name']} {row['row']} {row['col']}\"\n",
    "        if key in ids:\n",
    "            cell_value = key_to_cell[key]\n",
    "            data = ids[key]\n",
    "            mentions[cell_value] = data\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38394dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_mentions = sorted(mentions.items(), key=lambda x: x[1][\"ed_score\"])\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(sorted_mentions, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df2b9e56-0bcc-4cdb-9bb3-a23e505faf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998da0eb-bd65-4ce4-bd49-88610ce63963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ed_score and jaccard_score values\n",
    "ed_scores = [item[1]['ed_score'] for item in R4_sorted_mentions]\n",
    "jaccard_scores = [item[1]['jaccard_score'] for item in R4_sorted_mentions]\n",
    "\n",
    "# Convert to a pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame({'ED Score': ed_scores, 'Jaccard Score': jaccard_scores})\n",
    "\n",
    "# Density Plot for ED Score and Jaccard Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for ED Score\n",
    "sns.kdeplot(df['ED Score'], fill=True, label='ED Score')\n",
    "\n",
    "# Plot for Jaccard Score\n",
    "sns.kdeplot(df['Jaccard Score'], fill=True, label='Jaccard Score')\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ED and Jaccard Scores')\n",
    "plt.legend(loc='upper left')  # Show legend with labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673328d-fba3-48b5-8f8a-b74f67b7f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a6451-1289-489b-937f-a42891bd741a",
   "metadata": {},
   "source": [
    "## Sample extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b420df23-d18e-460b-9471-0915d141b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_sorted_mentions[:q1_idx]\n",
    "q2 = R4_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_sorted_mentions[q3_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ab86609-191e-4574-96e5-209b44bf850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = int(len(R4_sorted_mentions)/40) \n",
    "R4_sample_keys = []\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q1, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q2, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q3, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q4, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c115e-6e5b-4300-9b6a-6c7764c5a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting ED scores and Jaccard scores\n",
    "ed_scores = [score[1]['ed_score'] for score in R4_sample_keys]\n",
    "jaccard_scores = [score[1]['jaccard_score'] for score in R4_sample_keys]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.kdeplot(ed_scores, color='skyblue', label='Edit Distance Score', fill=True)\n",
    "sns.kdeplot(jaccard_scores, color='salmon', label='Jaccard Score', fill=True)\n",
    "\n",
    "plt.title('Edit Distance and Jaccard Score Density')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c307c-a769-4192-82b5-f9b4711b974e",
   "metadata": {},
   "source": [
    "## Coverage Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb8c75-5a9d-4d30-915d-95739e4d7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "434f331b-1795-4c53-b319-278f35be76f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 45074/46264 [43:26<01:08, 17.29it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of R1: 0.9742780563721252\n",
      "Measure Reciprocal Rank of R1: 0.9173043835383703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, el, string_name_list, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': str(el),\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{el}\", \"boost\": 2.0}}}}}}]}}}}}}'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{el}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', string_name_list[el])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(R4_sample_keys, url, pbar):\n",
    "    string_name_list = {item[1]['name']: item[1]['id'] for item in R4_sample_keys}\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for el in string_name_list.keys():\n",
    "            tasks.append(process_item(session, el, string_name_list, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for mrr_increment, count in results:\n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(R4_sample_keys)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(R4_sample_keys)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(R4_sample_keys))\n",
    "        asyncio.run(main(R4_sample_keys, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(R4_sample_keys, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d59524-0dda-462a-a27c-16f55864af53",
   "metadata": {},
   "source": [
    "### popularity based query \n",
    "Coverage of R1: 0.9742780563721252\n",
    "\r\n",
    "Measure Reciprocal Rank of R1: 0.9173043835383703"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8d5dc-9d79-4445-9fbf-68f30328c8a1",
   "metadata": {},
   "source": [
    "## Datasets Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2ea98-7025-484e-9cb3-d0cd69c20867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores(data):\n",
    "    return [item[1]['ed_score'] for item in data]\n",
    "\n",
    "ed_scores_R1 = extract_scores(R1_sample_keys)\n",
    "ed_scores_R3 = extract_scores(R3_sample_keys)\n",
    "ed_scores_R4 = extract_scores(R4_sample_keys)\n",
    "ed_scores_R4_2T = extract_scores(R4_2T_sample_keys)\n",
    "\n",
    "# Plot the KDE plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.kdeplot(ed_scores_R1, color='skyblue', label='R1 Edit Distance Score', fill=True)\n",
    "sns.kdeplot(ed_scores_R3, color='green', label='R3 Edit Distance Score', fill=True)\n",
    "sns.kdeplot(ed_scores_R4, color='red', label='R4 Edit Distance Score', fill=True)\n",
    "sns.kdeplot(ed_scores_R4_2T, color='purple', label='R4_2T Edit Distance Score', fill=True)\n",
    "\n",
    "plt.xlabel('Edit Distance Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Edit Distance Scores for Different Rounds')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
