{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c242f0f9-c9e3-4f3b-8d15-96050eeeb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import os\n",
    "import sys\n",
    "from pymongo import MongoClient\n",
    "from json.decoder import JSONDecodeError\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef38472-3087-45f2-ae9b-fdd8757076a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoClient(host=['mongo:27017'], document_class=dict, tz_aware=False, connect=True)\n"
     ]
    }
   ],
   "source": [
    "# MongoDB connection setup\n",
    "MONGO_ENDPOINT, MONGO_ENDPOINT_PORT = os.environ[\"MONGO_ENDPOINT\"].split(\":\")\n",
    "MONGO_ENDPOINT_PORT = int(MONGO_ENDPOINT_PORT)\n",
    "MONGO_ENDPOINT_USERNAME = os.environ[\"MONGO_INITDB_ROOT_USERNAME\"]\n",
    "MONGO_ENDPOINT_PASSWORD = os.environ[\"MONGO_INITDB_ROOT_PASSWORD\"]\n",
    "DB_NAME = f\"wikidata\"\n",
    "\n",
    "client = MongoClient(MONGO_ENDPOINT, MONGO_ENDPOINT_PORT, username=MONGO_ENDPOINT_USERNAME, password=MONGO_ENDPOINT_PASSWORD)\n",
    "print(client)\n",
    "\n",
    "log_c = client.wikidata.log\n",
    "items_c = client[DB_NAME].items\n",
    "objects_c = client[DB_NAME].objects\n",
    "literals_c = client[DB_NAME].literals\n",
    "types_c = client[DB_NAME].types\n",
    "\n",
    "c_ref = {\n",
    "    \"items\": items_c,\n",
    "    \"objects\":objects_c, \n",
    "    \"literals\":literals_c, \n",
    "    \"types\":types_c\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16daa576-49bf-4fd5-960f-a034684916a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush_buffer(buffer):\n",
    "    for key in buffer:\n",
    "        if len(buffer[key]) > 0:\n",
    "            c_ref[key].insert_many(buffer[key])\n",
    "            buffer[key] = []\n",
    "\n",
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ccbd4c-be3e-4b23-a8b8-d93e3f9397fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./def_mapping.json\"\n",
    "\n",
    "try:\n",
    "    # Open the JSON file for reading\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        mapping = json.load(json_file)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{json_file_path}' not found.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON data: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b855d5-e433-489e-a4fc-136eaba530e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_processed = 0\n",
    "num_entities_processed = 0\n",
    "\n",
    "def update_average_size(new_size):\n",
    "    global total_size_processed, num_entities_processed\n",
    "    total_size_processed += new_size\n",
    "    num_entities_processed += 1\n",
    "    return total_size_processed / num_entities_processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b8114a-6765-4c4c-ab18-1a5c17c8aee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/abelo/my-data/latest-all.json.bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m initial_estimated_average_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m800\u001b[39m\n\u001b[1;32m      3\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# Number of entities to insert in a single batch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m compressed_file_size \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwikidata_dump_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m initial_total_lines_estimate \u001b[38;5;241m=\u001b[39m compressed_file_size \u001b[38;5;241m/\u001b[39m initial_estimated_average_size\n\u001b[1;32m      7\u001b[0m DATATYPES_MAPPINGS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexternal-id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTRING\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtabular-data\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTABULAR_DATA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m }\n",
      "File \u001b[0;32m<frozen genericpath>:50\u001b[0m, in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/abelo/my-data/latest-all.json.bz2'"
     ]
    }
   ],
   "source": [
    "wikidata_dump_path = './data/latest-all.json.bz2'\n",
    "initial_estimated_average_size = 800\n",
    "BATCH_SIZE = 100 # Number of entities to insert in a single batch\n",
    "compressed_file_size = os.path.getsize(wikidata_dump_path)\n",
    "initial_total_lines_estimate = compressed_file_size / initial_estimated_average_size\n",
    "\n",
    "DATATYPES_MAPPINGS = {\n",
    "    'external-id': 'STRING',\n",
    "    'quantity': 'NUMBER',\n",
    "    'globe-coordinate': 'STRING',\n",
    "    'string': 'STRING',\n",
    "    'monolingualtext': 'STRING',\n",
    "    'commonsMedia': 'STRING',\n",
    "    'time': 'DATETIME',\n",
    "    'url': 'STRING',\n",
    "    'geo-shape': 'GEOSHAPE',\n",
    "    'math': 'MATH',\n",
    "    'musical-notation': 'MUSICAL_NOTATION',\n",
    "    'tabular-data': 'TABULAR_DATA'\n",
    "}\n",
    "DATATYPES = list(set(DATATYPES_MAPPINGS.values()))\n",
    "\n",
    "buffer = {\n",
    "    \"items\": [],\n",
    "    \"objects\": [], \n",
    "    \"literals\": [], \n",
    "    \"types\": []\n",
    "}\n",
    "\n",
    "def check_skip(obj, datatype):\n",
    "    temp = obj.get(\"mainsnak\", obj)\n",
    "    if \"datavalue\" not in temp:\n",
    "        return True\n",
    "\n",
    "    skip = {\n",
    "        \"wikibase-lexeme\",\n",
    "        \"wikibase-form\",\n",
    "        \"wikibase-sense\"\n",
    "    }\n",
    "\n",
    "    return datatype in skip\n",
    "\n",
    "\n",
    "def get_value(obj, datatype):\n",
    "    temp = obj.get(\"mainsnak\", obj)\n",
    "    if datatype == \"globe-coordinate\":\n",
    "        latitude = temp[\"datavalue\"][\"value\"][\"latitude\"]\n",
    "        longitude = temp[\"datavalue\"][\"value\"][\"longitude\"]\n",
    "        value = f\"{latitude},{longitude}\"\n",
    "    else:\n",
    "        keys = {\n",
    "            \"quantity\": \"amount\",\n",
    "            \"monolingualtext\": \"text\",\n",
    "            \"time\": \"time\",\n",
    "        }\n",
    "        if datatype in keys:\n",
    "            key = keys[datatype]\n",
    "            value = temp[\"datavalue\"][\"value\"][key]\n",
    "        else:\n",
    "            value = temp[\"datavalue\"][\"value\"]\n",
    "    return value\n",
    "\n",
    "global initial_total_lines_estimate\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "    count = 1000\n",
    "    \n",
    "    ORG = []\n",
    "    PERS = []\n",
    "    LOC = []\n",
    "    OTHERS = []\n",
    "\n",
    "    pbar = tqdm(total=initial_total_lines_estimate)\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            # Parse JSON data from each line\n",
    "            item = json.loads(line[:-2])\n",
    "\n",
    "            entity = item['id']\n",
    "            labels = item.get(\"labels\", {})\n",
    "            english_label = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "            aliases = item.get(\"aliases\", {})\n",
    "            description = item.get('descriptions', {}).get('en', {})\n",
    "            category = \"entity\"\n",
    "            sitelinks = item.get(\"sitelinks\", {})\n",
    "            popularity = len(sitelinks) if len(sitelinks) > 0 else 1\n",
    "\n",
    "            \n",
    "            if entity in list(mapping.values()):\n",
    "                all_labels = {}\n",
    "                for lang in labels:\n",
    "                    all_labels[lang] = labels[lang][\"value\"]\n",
    "            \n",
    "                all_aliases = {}\n",
    "                for lang in aliases:\n",
    "                    all_aliases[lang] = []\n",
    "                    for alias in aliases[lang]:\n",
    "                        all_aliases[lang].append(alias[\"value\"])\n",
    "                    all_aliases[lang] = list(set(all_aliases[lang]))\n",
    "            \n",
    "                for predicate in item[\"claims\"]:\n",
    "                    if predicate == \"P279\":\n",
    "                        category = \"type\"\n",
    "                        break\n",
    "                    if predicate == \"P31\":\n",
    "                        if 'Q4167410' == entity:\n",
    "                            category = \"disambiguation\"\n",
    "                            break\n",
    "                        elif 'Q4167836' == entity:\n",
    "                            category = \"category\"\n",
    "                            break\n",
    "\n",
    "                if entity[0] == \"P\":\n",
    "                    category = \"predicate\"\n",
    "        \n",
    "                line_size = len(line)\n",
    "                current_average_size = update_average_size(line_size)\n",
    "                pbar.total = round(compressed_file_size / current_average_size)\n",
    "                pbar.update(1)\n",
    "    \n",
    "                ###############################################################\n",
    "                # ORGANIZATION EXTRACTION\n",
    "                # All items with the root class Organization (Q43229) excluding country (Q6256), city (Q515), capitals (Q5119), \n",
    "                # administrative territorial entity of a single country (Q15916867), venue (Q17350442), sports league (Q623109) \n",
    "                # and family (Q8436)\n",
    "                \n",
    "                # LOCATION EXTRACTION\n",
    "                # All items with the root class Geographic Location (Q2221906) excluding: food (Q2095), educational institution (Q2385804), \n",
    "                # government agency (Q327333), international organization (Q484652) and time zone (Q12143)\n",
    "                \n",
    "                # PERSON EXTRACTION\n",
    "                # All items with the statement is instance of (P31) human (Q5) are classiﬁed as person.\n",
    "    \n",
    "                NERtype = None\n",
    "    \n",
    "                if item.get(\"type\") == \"item\" and \"claims\" in item:\n",
    "                    p31_claims = item[\"claims\"].get(\"P31\", [])\n",
    "                    \n",
    "                    if len(p31_claims) != 0:           \n",
    "                        for claim in p31_claims:\n",
    "                            mainsnak = claim.get(\"mainsnak\", {})\n",
    "                            datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                            numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "                            \n",
    "                            if numeric_id == 5:\n",
    "                                NERtype = \"PERS\" \n",
    "                            elif numeric_id in geolocation_subclass or any(k.lower() in description.get('value', '').lower() for k in [\"district\", \"city\", \"country\", \"capital\"]):\n",
    "                                NERtype = \"LOC\"\n",
    "                            elif numeric_id in organization_subclass:\n",
    "                                NERtype = \"ORG\"  \n",
    "                            else:\n",
    "                                NERtype = \"OTHERS\"\n",
    "                    else:\n",
    "                        NERtype = \"OTHERS\" \n",
    "                        \n",
    "                ################################################################   \n",
    "                ################################################################   \n",
    "                # URL EXTRACTION\n",
    "            \n",
    "                try:\n",
    "                    lang = labels.get(\"en\", {}).get(\"language\", \"\")\n",
    "                    tmp={}\n",
    "                    tmp[\"WD_id\"] = item['id']\n",
    "                    tmp[\"WP_id\"] = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "            \n",
    "                    url_dict={}\n",
    "                    url_dict[\"wikidata\"] = \"http://www.wikidata.org/wiki/\"+tmp[\"WD_id\"]\n",
    "                    url_dict[\"wikipedia\"] = \"http://\"+lang+\".wikipedia.org/wiki/\"+tmp[\"WP_id\"].replace(\" \",\"_\")\n",
    "                    url_dict[\"dbpedia\"] = \"http://dbpedia.org/resource/\"+tmp[\"WP_id\"].capitalize().replace(\" \",\"_\")\n",
    "                    \n",
    "            \n",
    "                except json.decoder.JSONDecodeError:\n",
    "                   pass\n",
    "                \n",
    "                ################################################################    \n",
    "        \n",
    "                objects = {}\n",
    "                literals = {datatype: {} for datatype in DATATYPES}\n",
    "                types = {\"P31\": []}\n",
    "                join = {\n",
    "                    \"items\": {\n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"description\": description,\n",
    "                        \"labels\": all_labels,\n",
    "                        \"aliases\": all_aliases,\n",
    "                        \"types\": types,\n",
    "                        \"popularity\": popularity,\n",
    "                        \"kind\": category,   # kind (entity, type or predicate, disambiguation or category)\n",
    "                        ######################\n",
    "                        # new updates\n",
    "                        \"NERtype\": NERtype, # (ORG, LOC, PER or OTHERS)\n",
    "                        \"URLs\" : url_dict\n",
    "                        ######################\n",
    "                    },\n",
    "                    \"objects\": { \n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"objects\":objects\n",
    "                    },\n",
    "                    \"literals\": { \n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"literals\": literals\n",
    "                    },\n",
    "                    \"types\": { \n",
    "                        \"id_entity\": i,\n",
    "                        \"entity\": entity,\n",
    "                        \"types\": types\n",
    "                    },\n",
    "                }\n",
    "            \n",
    "                predicates = item[\"claims\"]\n",
    "                for predicate in predicates:\n",
    "                    for obj in predicates[predicate]:\n",
    "                        datatype = obj[\"mainsnak\"][\"datatype\"]\n",
    "            \n",
    "                        if check_skip(obj, datatype):\n",
    "                            continue\n",
    "            \n",
    "                        if datatype == \"wikibase-item\" or datatype == \"wikibase-property\":\n",
    "                            value = obj[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "            \n",
    "                            if predicate == \"P31\" or predicate == \"P106\":\n",
    "                                types[\"P31\"].append(value)\n",
    "            \n",
    "                            if value not in objects:\n",
    "                                objects[value] = []\n",
    "                            objects[value].append(predicate)    \n",
    "                        else:\n",
    "                            value = get_value(obj, datatype)                \n",
    "                            lit = literals[DATATYPES_MAPPINGS[datatype]]\n",
    "            \n",
    "                            if predicate not in lit:\n",
    "                                lit[predicate] = []\n",
    "                            lit[predicate].append(value)   \n",
    "            \n",
    "                 \n",
    "            \n",
    "                for key in buffer:\n",
    "                    buffer[key].append(join[key])            \n",
    "            \n",
    "                if len(buffer[\"items\"]) == BATCH_SIZE:\n",
    "                    flush_buffer(buffer)\n",
    "    \n",
    "        except json.decoder.JSONDecodeError:\n",
    "            continue\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a06f6-0932-4c54-9abc-714df8f1709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./yago_wiki_classification.json\"\n",
    "\n",
    "data = {\n",
    "    \"ORG\": ORG,\n",
    "    \"LOC\": LOC,\n",
    "    \"PERS\": PERS,\n",
    "    \"OTHERS\": OTHERS\n",
    "}\n",
    "\n",
    "# Write the categorized data to a JSON file\n",
    "try:\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"Data saved successfully to {json_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6043e2-23dc-4a5d-8f49-9c8f6b598523",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = parser.parse_all_logs(log_dir=\"./\")\n",
    "first_log = logs[0]\n",
    "\n",
    "print(f\"Output file name: {first_log['output_filename']}\")\n",
    "print(f\"Standard file name: {first_log['standard_filename']}\")\n",
    "print(f\"Stopped early: {first_log['early_stop']}\")\n",
    "print(f\"Measured consumption: {first_log['actual']}\")\n",
    "print(f\"Predicted consumption: {first_log['pred']}\")\n",
    "print(f\"Measured GPU devices: {first_log['components']['gpu']['devices']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a850f-8f23-4096-9a22-e594d6ece098",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length_PERS = len(PERS)\n",
    "total_length_ORG = len(ORG)\n",
    "total_length_LOC = len(LOC)\n",
    "total_length_OTHERS = len(OTHERS)\n",
    "\n",
    "# Print the total lengths\n",
    "print(\"Total lengths:\")\n",
    "print(f\"Length of PERS: {total_length_PERS}\")\n",
    "print(f\"Length of ORG: {total_length_ORG}\")\n",
    "print(f\"Length of LOC: {total_length_LOC}\")\n",
    "print(f\"Length of OTHERS: {total_length_OTHERS}\")\n",
    "\n",
    "# Calculate the sum of lengths\n",
    "total_length = total_length_PERS + total_length_ORG + total_length_LOC + total_length_OTHERS\n",
    "\n",
    "# Print the sum of lengths\n",
    "print(f\"Total length: {total_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a9fd3-190e-43d4-93a6-acb0d79af0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in OTHERS:\n",
    "    if el in PERS:\n",
    "        print(f\"PERS and ORG --> Entity ID: {PERS.index(el)}\")\n",
    "    if el in LOC:\n",
    "        print(f\"LOC and ORG --> Entity ID: {LOC.index(el)}\")\n",
    "    if el in ORG:\n",
    "        print(f\"OTHERS and ORG --> Entity ID: {ORG.index(el)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1273d4-71d2-4e5d-9100-48ede8cc4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to sets for faster intersection operation\n",
    "ORG_set = set(ORG)\n",
    "PERS_set = set(PERS)\n",
    "LOC_set = set(LOC)\n",
    "OTHERS_set = set(OTHERS)\n",
    "\n",
    "# Initialize counters for each set\n",
    "ORG_counter = 0\n",
    "PERS_counter = 0\n",
    "LOC_counter = 0\n",
    "OTHERS_counter = 0\n",
    "\n",
    "# Find the overlapping items and update the counters\n",
    "for item in ORG_set.union(PERS_set, LOC_set, OTHERS_set):\n",
    "    num_overlaps = 0\n",
    "    if item in ORG_set:\n",
    "        print(\"item\")\n",
    "        num_overlaps += 1\n",
    "    if item in PERS_set:\n",
    "        num_overlaps += 1\n",
    "    if item in LOC_set:\n",
    "        num_overlaps += 1\n",
    "    if item in OTHERS_set:\n",
    "        num_overlaps += 1\n",
    "    \n",
    "    # Update the corresponding counter based on the number of overlaps\n",
    "    if num_overlaps == 1:\n",
    "        ORG_counter += 1\n",
    "    elif num_overlaps == 2:\n",
    "        PERS_counter += 1\n",
    "    elif num_overlaps == 3:\n",
    "        LOC_counter += 1\n",
    "    elif num_overlaps == 4:\n",
    "        OTHERS_counter += 1\n",
    "\n",
    "# Print the counts for each set\n",
    "print(\"Number of overlaps for each set:\")\n",
    "print(f\"ORG: {ORG_counter}\")\n",
    "print(f\"PERS: {PERS_counter}\")\n",
    "print(f\"LOC: {LOC_counter}\")\n",
    "print(f\"OTHERS: {OTHERS_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c9245",
   "metadata": {},
   "source": [
    "## URL Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4029e46-84a4-4177-a31b-e228d4149814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# This Python file uses the following encoding: utf-8\n",
    "\n",
    "__author__ = 'jgeiss'\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# authors: Johanna Geiß, Heidelberg University, Germany                     #\n",
    "# email: geiss@informatik.uni-heidelberg.de                                 #\n",
    "# Copyright (c) 2017 Database Research Group,                               #\n",
    "#               Institute of Computer Science,                              #\n",
    "#               University of Heidelberg                                    #\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");         #\n",
    "#   you may not use this file except in compliance with the License.        #\n",
    "#   You may obtain a copy of the License at                                 #\n",
    "#                                                                           #\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0                              #\n",
    "#                                                                           #\n",
    "#   Unless required by applicable law or agreed to in writing, software     #\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,       #\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.#\n",
    "#   See the License for the specific language governing permissions and     #\n",
    "#   limitations under the License.                                          #\n",
    "#############################################################################\n",
    "# last updated 21.3.2017 by Johanna Geiß\n",
    "\n",
    "from pymongo import *\n",
    "from pymongo import errors\n",
    "import configparser\n",
    "\n",
    "\n",
    "\n",
    "wikidata_dump_path = './my-data/latest-all.json.bz2'\n",
    "\n",
    "with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "    count = 0\n",
    "    \n",
    "             \n",
    "    for i, line in tqdm(enumerate(f), total=1000):\n",
    "        if count == 10000:\n",
    "            break\n",
    "        try:\n",
    "            count += 1\n",
    "            # Parse JSON data from each line\n",
    "            data = json.loads(line[:-2])\n",
    "         \n",
    "            labels = data.get(\"labels\", {})\n",
    "            lang = labels.get(\"en\", {}).get(\"language\", \"\")\n",
    "            entry={}\n",
    "            entry[\"WD_id\"] = data['id']\n",
    "            entry[\"WP_id\"] = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "\n",
    "            entry[\"WD_id_URL\"] = \"http://www.wikidata.org/wiki/\"+entry[\"WD_id\"]\n",
    "            entry[\"WP_id_URL\"] = \"http://\"+lang+\".wikipedia.org/wiki/\"+entry[\"WP_id\"].replace(\" \",\"_\")\n",
    "            entry[\"dbpedia_URL\"] = \"http://dbpedia.org/resource/\"+entry[\"WP_id\"].capitalize().replace(\" \",\"_\")\n",
    "            \n",
    "            print(\"------------------\")\n",
    "            print(entry[\"WD_id_URL\"])\n",
    "            print(entry[\"WP_id_URL\"])\n",
    "            print(entry[\"dbpedia_URL\"])\n",
    "            print(\"------------------\")\n",
    "    \n",
    "        except json.decoder.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4405d-3f61-4355-ac23-5e685e372807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbontracker import parser\n",
    "\n",
    "logs = parser.parse_all_logs(log_dir=\"./\")\n",
    "print(logs)\n",
    "first_log = logs[0]\n",
    "\n",
    "print(f\"Output file name: {first_log['output_filename']}\")\n",
    "print(f\"Standard file name: {first_log['standard_filename']}\")\n",
    "print(f\"Stopped early: {first_log['early_stop']}\")\n",
    "print(f\"Measured consumption: {first_log['actual']}\")\n",
    "print(f\"Predicted consumption: {first_log['pred']}\")\n",
    "print(f\"Measured GPU devices: {first_log['components']['gpu']['devices']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c39bc5-a679-46c2-8406-0726fc6737cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
